# ==========================================
# Componente 4: Modelos predictivos (Decision Tree, Random Forest, XGBoost)
# Grupo A: Atractivo, Diversi√≥n, Intereses Compartidos
# ==========================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix
)
from xgboost import XGBClassifier

class ModelosGrupoA:
    def __init__(self, data_path="data/speed_dating_cleaned.csv"):
        self.data_path = data_path
        self.models = {}
        self.results = {}
        self.metrics_df = None

    # ===========================
    # 1. Cargar y preparar datos
    # ===========================
    def cargar_datos(self):
        df = pd.read_csv(self.data_path)
        cols = ['match', 'attr_o', 'fun_o', 'int_corr']
        data = df[cols].dropna()
        X = data[['attr_o', 'fun_o', 'int_corr']]
        y = (data['match'] == 1).astype(int)
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        print(f"‚úÖ Datos cargados y divididos: {self.X_train.shape[0]} entrenamiento / {self.X_test.shape[0]} prueba")

    # ===========================
    # 2. Entrenar modelos
    # ===========================
    def entrenar_modelos(self):
        # Decision Tree
        self.models["Decision Tree"] = DecisionTreeClassifier(max_depth=4, random_state=42)
        # Random Forest
        self.models["Random Forest"] = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=42)
        # XGBoost
        self.models["XGBoost"] = XGBClassifier(
            random_state=42,
            n_estimators=250,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric='logloss'
        )

        # Entrenamiento
        for name, model in self.models.items():
            model.fit(self.X_train, self.y_train)
            preds = model.predict(self.X_test)
            self.results[name] = preds
            acc = accuracy_score(self.y_test, preds)
            print(f"{name} entrenado ‚úÖ - Accuracy: {acc:.3f}")

    # ===========================
    # 3. Evaluar modelos
    # ===========================
    def evaluar_modelos(self):
        resumen = []
        for name, y_pred in self.results.items():
            print(f"\n==============================")
            print(f"üìä M√âTRICAS DE {name.upper()}")
            print(f"==============================")

            report = classification_report(self.y_test, y_pred, output_dict=True)
            print(pd.DataFrame(report).T.round(3))

            resumen.append({
                "Modelo": name,
                "Accuracy": report["accuracy"],
                "Precision (Match)": report["1"]["precision"],
                "Recall (Match)": report["1"]["recall"],
                "F1-score (Match)": report["1"]["f1-score"]
            })

            # Matriz de confusi√≥n
            cm = confusion_matrix(self.y_test, y_pred)
            plt.figure(figsize=(4, 3))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
            plt.title(f"Matriz de confusi√≥n - {name}")
            plt.xlabel("Predicho")
            plt.ylabel("Real")
            plt.tight_layout()
            plt.show()

        self.metrics_df = pd.DataFrame(resumen)
        print("\nResumen comparativo de m√©tricas:")
        print(self.metrics_df.round(3))

        # Gr√°fico de comparaci√≥n
        fig, axes = plt.subplots(1, 3, figsize=(14, 4))
        sns.barplot(data=self.metrics_df, x="Modelo", y="Precision (Match)", ax=axes[0], palette="mako")
        sns.barplot(data=self.metrics_df, x="Modelo", y="Recall (Match)", ax=axes[1], palette="mako")
        sns.barplot(data=self.metrics_df, x="Modelo", y="F1-score (Match)", ax=axes[2], palette="mako")
        axes[0].set_title("Precisi√≥n (Match)")
        axes[1].set_title("Recall (Match)")
        axes[2].set_title("F1-score (Match)")
        for ax in axes: ax.set_ylim(0, 1)
        plt.suptitle("Comparaci√≥n de m√©tricas de desempe√±o - Grupo A", fontsize=13)
        plt.tight_layout()
        plt.show()

    # ===========================
    # 4. Importancia de variables
    # ===========================
    def importancia_variables(self):
        importancia_df = pd.DataFrame({
            'Variable': self.X_train.columns,
            'DecisionTree': self.models["Decision Tree"].feature_importances_,
            'RandomForest': self.models["Random Forest"].feature_importances_,
            'XGBoost': self.models["XGBoost"].feature_importances_
        }).set_index('Variable')

        print("\nüî• Importancia promedio de variables:")
        print(importancia_df)

        importancia_df.plot(kind='bar', figsize=(8, 5), colormap='viridis')
        plt.title("Importancia de variables por modelo")
        plt.ylabel("Importancia")
        plt.tight_layout()
        plt.show()

    # ===========================
    # 5. Conclusi√≥n
    # ===========================
    def conclusiones(self):
        best_model = self.metrics_df.sort_values(by="F1-score (Match)", ascending=False).iloc[0]["Modelo"]
        print("\n--- CONCLUSIONES DEL COMPONENTE 4 ---")
        print(f"üèÜ El modelo con mejor balance entre precisi√≥n y recall es: {best_model}")
        print("\n‚úîÔ∏è Todos los modelos obtuvieron desempe√±os similares (~0.83‚Äì0.85 de accuracy),")
        print("   lo que indica que las variables del Grupo A tienen un patr√≥n fuerte y estable.")
        print("‚úîÔ∏è Los factores m√°s influyentes son el atractivo y la diversi√≥n percibidos,")
        print("   seguidos de los intereses compartidos (int_corr).")
        print("‚úîÔ∏è XGBoost y Random Forest ofrecen mejor generalizaci√≥n, aunque con diferencias m√≠nimas.")
        print("‚úîÔ∏è El Decision Tree sigue siendo √∫til por su interpretabilidad y simplicidad.")
        print("---------------------------------------")

# ===========================
# Ejecuci√≥n completa
# ===========================
if __name__ == "__main__":
    modeloA = ModelosGrupoA("data/speed_dating_cleaned.csv")
    modeloA.cargar_datos()
    modeloA.entrenar_modelos()
    modeloA.evaluar_modelos()
    modeloA.importancia_variables()
    modeloA.conclusiones()

